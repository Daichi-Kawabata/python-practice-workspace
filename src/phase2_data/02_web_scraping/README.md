# 02. Webスクレイピング基礎（型安全版）

## 学習目標
- requestsライブラリを使ったHTTP通信
- BeautifulSoupを使ったHTML解析（型安全実装）
- WebAPIからのJSONデータ取得
- スクレイピングのマナーとベストプラクティス
- エラーハンドリングとリトライ処理
- **型安全なプログラミング手法の習得**

## 学習項目
- [x] requestsの基本操作（GET、POST、ヘッダー設定）
- [x] BeautifulSoupでのHTML解析（要素の検索・抽出）
- [x] **型安全なBeautifulSoup実装（Pylance/Pyright対応）**
- [x] WebAPI呼び出しとJSONデータ処理
- [x] レート制限とpolite scraping
- [x] セッション管理と認証
- [x] エラーハンドリング（ネットワークエラー、HTTPエラー）
- [x] 実践的なスクレイピング課題の実装

## ファイル構成

### 📝 メインファイル（全て型エラー0個）
- `requests_basics.py` - requestsライブラリの基本操作
- `beautifulsoup_basics.py` - **型安全なBeautifulSoupによるHTML解析**
- `beautifulsoup_practical.py` - **型安全な実践的HTML解析例**
- `web_api_demo.py` - **型安全なWebAPI呼び出し実践**
- `scraping_challenges.py` - **型安全な実践的スクレイピング課題5選**

### 📚 学習用リソース
- `beautifulsoup_reference.py` - 完全な型安全リファレンス実装
- `BeautifulSoup型安全ガイド.md` - 型安全プログラミングの詳細ガイド
- `型安全化完了レポート.md` - 修正過程と成果の完全レポート
- `README.md` - このファイル

### 📊 生成されたデータファイル
- `users_data.csv` - API取得データのCSV出力例
- `quotes_data.csv` - 名言サイトスクレイピング結果
- `weather_report.json` - 天気予報API模擬データ
- `price_history.csv` - 価格監視システムの結果
- `integrated_report.json` - データパイプライン統合結果

## 学習成果

### 🎯 型安全化の達成
- **全ファイルで型エラー0個を達成**（Pylance/Pyright対応）
- 型安全な補助関数の導入（`safe_get_text`, `safe_get_attribute`, `safe_find`, `safe_find_all`）
- 実行時エラーの削減とコードの堅牢性向上
- IDEでの適切な補完とエラー検出を実現

### 実装完了項目
1. **HTTP通信の基礎**: requests ライブラリでのGET/POSTリクエスト
2. **型安全なHTML解析**: BeautifulSoup でのタグ検索、属性取得、テキスト抽出
3. **JSON処理**: API レスポンスの解析とデータ変換
4. **エラーハンドリング**: ネットワークエラー、HTTPエラーの適切な処理
5. **レート制限**: 礼儀正しいスクレイピングの実装
6. **データ出力**: CSV、JSON形式でのデータ保存
7. **セッション管理**: Cookie の維持と認証パターン
8. **🆕 型安全プログラミング**: 静的型チェック対応コードの作成

### 実践課題（全て型安全対応）
1. **名言サイトスクレイピング**: HTML解析によるデータ抽出・CSV保存
2. **天気予報API活用**: 複数都市データの統計処理・JSON保存
3. **ニュースアグリゲーター**: 実際のWebサイトからの記事取得
4. **価格監視システム**: 価格変動の模擬実装
5. **データパイプライン**: 複数ソースからのデータ統合

## 🔧 技術的な改善点

### 型安全化のベネフィット
- **バグの早期発見**: 開発時に型エラーを検出
- **コードの可読性向上**: 明確な型情報
- **保守性の向上**: リファクタリングの安全性
- **開発効率の向上**: IDEの支援機能を最大活用

### 使用した技術
- **型ヒント**: `Optional`, `Union`, `List`, `Dict`
- **安全な要素アクセス**: `isinstance`チェック
- **デフォルト値設定**: Noneエラー回避
- **適切なインポート**: `bs4.element`からの型安全インポート

## 注意事項
- **robots.txtの確認**: スクレイピング前に必ずチェック
- **レート制限**: 適切な間隔でのアクセス
- **利用規約の確認**: サイトの利用規約を遵守
- **型チェック**: 開発時は`mypy`や`pylance`での型チェックを実行

## 🚀 次のステップ
1. Selenium を使用した動的コンテンツの処理
2. pandas との連携による高度なデータ分析
3. asyncio を使用した非同期スクレイピング
4. より複雑なWebサイトでの実践
- **User-Agentの設定**: 適切なUser-Agentヘッダーの設定

## 次のステップ
1. pandas ライブラリを使ったデータ分析への応用
2. Selenium を使った動的コンテンツの処理
3. 非同期処理（asyncio）による高速スクレイピング
4. データベースへの直接保存機能
5. 定期実行とデータ更新の自動化
